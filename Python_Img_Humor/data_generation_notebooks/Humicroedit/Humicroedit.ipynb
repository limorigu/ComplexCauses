{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Humicroedit dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import torch\n",
    "# from torchvision import datasets\n",
    "# from torch import nn, optim, autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "# from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "import random\n",
    "# from sklearn import svm\n",
    "# from sklearn import linear_model\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gensim, logging\n",
    "import ast\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "from cleantext import clean\n",
    "# import matplotlib.cm as cm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: to run this notebook you would need to download:\n",
    "\n",
    "1) Humicroedit data (from https://www.cs.rochester.edu/u/nhossain/humicroedit.html, Full Dataset Release)\n",
    "\n",
    "2) glove.6B (from https://nlp.stanford.edu/projects/glove/) \n",
    "\n",
    "and place them in the relevant paths below (in ../../data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_task1 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-1/train.csv\")\n",
    "train_funline_df_task1 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-1/train_funlines.csv\")\n",
    "val_df_task1 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-1/dev.csv\")\n",
    "test_df_task1 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-1/test.csv\")\n",
    "\n",
    "train_df_task2 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-2/train.csv\")\n",
    "train_funline_df_task2 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-2/train_funlines.csv\")\n",
    "val_df_task2 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-2/dev.csv\")\n",
    "test_df_task2 = pd.read_csv(\"../../data/semeval-2020-task-7-dataset/subtask-2/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_convert_to_task1(df):\n",
    "    dataset1 = df[['original1', 'edit1', 'grades1', 'meanGrade1']]\n",
    "    dataset2 = df[['original2', 'edit2', 'grades2', 'meanGrade2']]\n",
    "    dataset2.rename(columns={\"original2\": \"original1\", \"edit2\": \"edit1\", \"grades2\": \"grades1\", \"meanGrade2\": \"meanGrade1\"}, inplace=True)\n",
    "    combined = dataset1.append(dataset2)\n",
    "    combined.rename(columns={\"original1\": \"original\", \"edit1\": \"edit\", \"grades1\": \"grades\", \"meanGrade1\": \"meanGrade\"}, inplace=True)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lgultchin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## https://github.com/utkuozbulak/unsupervised-learning-document-clustering/blob/master/src/read_and_clean_documents.py\n",
    "def clean_string(st):\n",
    "    stemmer = PorterStemmer()\n",
    "    items_to_clean = set(list(stopwords.words('english')) + ['\\n','\\n\\n','\\n\\n\\n','\\n\\n\\n\\n','ocroutput','',' '])\n",
    "    # Items to clean\n",
    "    regex_non_alphanumeric = re.compile('[^0-9a-zA-Z]')  # REGEX for non alphanumeric chars\n",
    "    st = \" \".join(regex_non_alphanumeric.sub(' ', st).split())  # Filter text, remove non alphanumeric chars\n",
    "    st = st.lower()  # Lowercase the text\n",
    "    st = stemmer.stem(st)  # Stem the text\n",
    "    if len(st) < 3:  # If the length of item is lower than 3, remove item\n",
    "        item = ''\n",
    "    st = \" \".join([elem for elem in st.split(\" \") if elem not in items_to_clean])\n",
    "\n",
    "    return st\n",
    "\n",
    "def perform_edit(x):\n",
    "    first_part = x['original'].split(\"<\")[0]\n",
    "    second_part = x['original'].split(\">\")[1]\n",
    "    edit = x['edit']\n",
    "    result = first_part + edit + second_part\n",
    "#     return clean_string(result)\n",
    "    return result\n",
    "\n",
    "def replaced_word(x):\n",
    "    return x[x.find(\"<\")+len(\"<\"):x.rfind(\"/>\")]\n",
    "\n",
    "def restructure_dataset(df):\n",
    "    df['Z_raw'] = df['original'].apply(lambda x: x.replace(\"<\",\"\").replace(\"/>\",\"\"))\n",
    "#     df['Z_raw'] = df['original'].apply(lambda x: clean_string(x.replace(\"<\",\"\").replace(\"/>\",\"\")))\n",
    "    df['replaced'] = df['original'].apply(lambda x: replaced_word(x))\n",
    "    df['W_raw'] = df['edit']\n",
    "    df['X_raw'] = df.apply(lambda x: perform_edit(x), axis=1)\n",
    "    df['Y'] = df['meanGrade']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_task1 = train_df_task1.append(train_funline_df_task1).append(val_df_task1).append(test_df_task1)\n",
    "assert len(total_df_task1) == \\\n",
    "len(train_df_task1) + len(train_funline_df_task1) + \\\n",
    "len(val_df_task1) + len(test_df_task1)\n",
    "\n",
    "total_df_task2 = train_df_task2.append(train_funline_df_task2).append(val_df_task2).append(test_df_task2)\n",
    "assert len(total_df_task2) == \\\n",
    "len(train_df_task2) + len(train_funline_df_task2) + \\\n",
    "len(val_df_task2) + len(test_df_task2)\n",
    "total_df_task2 = task2_convert_to_task1(total_df_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56651\n",
      "41832\n"
     ]
    }
   ],
   "source": [
    "total_df = total_df_task1.append(total_df_task2)\n",
    "print(len(total_df))\n",
    "total_df.drop_duplicates(inplace=True)\n",
    "print(len(total_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530.0</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034.0</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>33110</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731.0</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.0</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6164.0</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuckerberg sold nearly $ 500 million Facebook ...</td>\n",
       "      <td>emojis</td>\n",
       "      <td>32210</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Fake &lt;news/&gt; ” or free speech : Is Google cr...</td>\n",
       "      <td>discs</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Stop this bullshit ” : uncle of Pakistani gi...</td>\n",
       "      <td>startled</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ The fish rots from the &lt;head/&gt; ” : a histori...</td>\n",
       "      <td>toupee</td>\n",
       "      <td>32110</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Trump ’s problem child ” : Mark Levin , Alex...</td>\n",
       "      <td>purchase</td>\n",
       "      <td>33221</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41832 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           original      edit  \\\n",
       "0     14530.0  France is ‘ hunting down its citizens who join...     twins   \n",
       "1     13034.0  Pentagon claims 2,000 % increase in Russian tr...   bowling   \n",
       "2      8731.0  Iceland PM Calls Snap Vote as Pedophile Furor ...     party   \n",
       "3        76.0  In an apparent first , Iran and Israel <engage...      slap   \n",
       "4      6164.0  Trump was told weeks ago that Flynn misled <Vi...    school   \n",
       "...       ...                                                ...       ...   \n",
       "2946      NaN  Zuckerberg sold nearly $ 500 million Facebook ...    emojis   \n",
       "2949      NaN  “ Fake <news/> ” or free speech : Is Google cr...     discs   \n",
       "2952      NaN  “ Stop this bullshit ” : uncle of Pakistani gi...  startled   \n",
       "2955      NaN  “ The fish rots from the <head/> ” : a histori...    toupee   \n",
       "2958      NaN  “ Trump ’s problem child ” : Mark Levin , Alex...  purchase   \n",
       "\n",
       "      grades  meanGrade  \n",
       "0      10000        0.2  \n",
       "1      33110        1.6  \n",
       "2      22100        1.0  \n",
       "3      20000        0.4  \n",
       "4          0        0.0  \n",
       "...      ...        ...  \n",
       "2946   32210        1.6  \n",
       "2949       0        0.0  \n",
       "2952       0        0.0  \n",
       "2955   32110        1.4  \n",
       "2958   33221        2.2  \n",
       "\n",
       "[41832 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>Z_raw</th>\n",
       "      <th>replaced</th>\n",
       "      <th>W_raw</th>\n",
       "      <th>X_raw</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530.0</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>Isis</td>\n",
       "      <td>twins</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034.0</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>33110</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>Syria</td>\n",
       "      <td>bowling</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731.0</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>Coalition</td>\n",
       "      <td>party</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.0</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>In an apparent first , Iran and Israel engage ...</td>\n",
       "      <td>engage</td>\n",
       "      <td>slap</td>\n",
       "      <td>In an apparent first , Iran and Israel slap ea...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6164.0</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled Vic...</td>\n",
       "      <td>Vice</td>\n",
       "      <td>school</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled sch...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuckerberg sold nearly $ 500 million Facebook ...</td>\n",
       "      <td>emojis</td>\n",
       "      <td>32210</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Zuckerberg sold nearly $ 500 million Facebook ...</td>\n",
       "      <td>stock</td>\n",
       "      <td>emojis</td>\n",
       "      <td>Zuckerberg sold nearly $ 500 million Facebook ...</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Fake &lt;news/&gt; ” or free speech : Is Google cr...</td>\n",
       "      <td>discs</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>“ Fake news ” or free speech : Is Google crack...</td>\n",
       "      <td>news</td>\n",
       "      <td>discs</td>\n",
       "      <td>“ Fake discs ” or free speech : Is Google crac...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Stop this bullshit ” : uncle of Pakistani gi...</td>\n",
       "      <td>startled</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>“ Stop this bullshit ” : uncle of Pakistani gi...</td>\n",
       "      <td>killed</td>\n",
       "      <td>startled</td>\n",
       "      <td>“ Stop this bullshit ” : uncle of Pakistani gi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ The fish rots from the &lt;head/&gt; ” : a histori...</td>\n",
       "      <td>toupee</td>\n",
       "      <td>32110</td>\n",
       "      <td>1.4</td>\n",
       "      <td>“ The fish rots from the head ” : a historian ...</td>\n",
       "      <td>head</td>\n",
       "      <td>toupee</td>\n",
       "      <td>“ The fish rots from the toupee ” : a historia...</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>NaN</td>\n",
       "      <td>“ Trump ’s problem child ” : Mark Levin , Alex...</td>\n",
       "      <td>purchase</td>\n",
       "      <td>33221</td>\n",
       "      <td>2.2</td>\n",
       "      <td>“ Trump ’s problem child ” : Mark Levin , Alex...</td>\n",
       "      <td>embrace</td>\n",
       "      <td>purchase</td>\n",
       "      <td>“ Trump ’s problem child ” : Mark Levin , Alex...</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41832 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           original      edit  \\\n",
       "0     14530.0  France is ‘ hunting down its citizens who join...     twins   \n",
       "1     13034.0  Pentagon claims 2,000 % increase in Russian tr...   bowling   \n",
       "2      8731.0  Iceland PM Calls Snap Vote as Pedophile Furor ...     party   \n",
       "3        76.0  In an apparent first , Iran and Israel <engage...      slap   \n",
       "4      6164.0  Trump was told weeks ago that Flynn misled <Vi...    school   \n",
       "...       ...                                                ...       ...   \n",
       "2946      NaN  Zuckerberg sold nearly $ 500 million Facebook ...    emojis   \n",
       "2949      NaN  “ Fake <news/> ” or free speech : Is Google cr...     discs   \n",
       "2952      NaN  “ Stop this bullshit ” : uncle of Pakistani gi...  startled   \n",
       "2955      NaN  “ The fish rots from the <head/> ” : a histori...    toupee   \n",
       "2958      NaN  “ Trump ’s problem child ” : Mark Levin , Alex...  purchase   \n",
       "\n",
       "      grades  meanGrade                                              Z_raw  \\\n",
       "0      10000        0.2  France is ‘ hunting down its citizens who join...   \n",
       "1      33110        1.6  Pentagon claims 2,000 % increase in Russian tr...   \n",
       "2      22100        1.0  Iceland PM Calls Snap Vote as Pedophile Furor ...   \n",
       "3      20000        0.4  In an apparent first , Iran and Israel engage ...   \n",
       "4          0        0.0  Trump was told weeks ago that Flynn misled Vic...   \n",
       "...      ...        ...                                                ...   \n",
       "2946   32210        1.6  Zuckerberg sold nearly $ 500 million Facebook ...   \n",
       "2949       0        0.0  “ Fake news ” or free speech : Is Google crack...   \n",
       "2952       0        0.0  “ Stop this bullshit ” : uncle of Pakistani gi...   \n",
       "2955   32110        1.4  “ The fish rots from the head ” : a historian ...   \n",
       "2958   33221        2.2  “ Trump ’s problem child ” : Mark Levin , Alex...   \n",
       "\n",
       "       replaced     W_raw                                              X_raw  \\\n",
       "0          Isis     twins  France is ‘ hunting down its citizens who join...   \n",
       "1         Syria   bowling  Pentagon claims 2,000 % increase in Russian tr...   \n",
       "2     Coalition     party  Iceland PM Calls Snap Vote as Pedophile Furor ...   \n",
       "3        engage      slap  In an apparent first , Iran and Israel slap ea...   \n",
       "4          Vice    school  Trump was told weeks ago that Flynn misled sch...   \n",
       "...         ...       ...                                                ...   \n",
       "2946      stock    emojis  Zuckerberg sold nearly $ 500 million Facebook ...   \n",
       "2949       news     discs  “ Fake discs ” or free speech : Is Google crac...   \n",
       "2952     killed  startled  “ Stop this bullshit ” : uncle of Pakistani gi...   \n",
       "2955       head    toupee  “ The fish rots from the toupee ” : a historia...   \n",
       "2958    embrace  purchase  “ Trump ’s problem child ” : Mark Levin , Alex...   \n",
       "\n",
       "        Y  \n",
       "0     0.2  \n",
       "1     1.6  \n",
       "2     1.0  \n",
       "3     0.4  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "2946  1.6  \n",
       "2949  0.0  \n",
       "2952  0.0  \n",
       "2955  1.4  \n",
       "2958  2.2  \n",
       "\n",
       "[41832 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = restructure_dataset(total_df)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw text columns to w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"../../data/glove.6B/glove.6B.300d.txt\", \n",
    "               word2vec_output_file=\"../../data/glove.6B/gensim_glove_vectors.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"../../data/glove.6B/gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(word2vec_model, word):\n",
    "    try:\n",
    "        return word2vec_model[word]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "    \n",
    "def edits_diff(x,model=model):\n",
    "    try:\n",
    "        edit = model[x['edit']]\n",
    "        init = model[x['replaced']]\n",
    "        return edit-init\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "    \n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    if len(doc)>0:\n",
    "        return np.mean(word2vec_model[doc], axis=0)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 3000000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_model.vocab.keys()), len(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 19.1 s, total: 1min 38s\n",
      "Wall time: 3min 21s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>Z_raw</th>\n",
       "      <th>replaced</th>\n",
       "      <th>W_raw</th>\n",
       "      <th>X_raw</th>\n",
       "      <th>Y</th>\n",
       "      <th>W_vec</th>\n",
       "      <th>Z_vec</th>\n",
       "      <th>X_vec</th>\n",
       "      <th>W_init_sub_edit_vec</th>\n",
       "      <th>W_glv_vec</th>\n",
       "      <th>Z_glv_vec</th>\n",
       "      <th>X_glv_vec</th>\n",
       "      <th>W_init_sub_edit_glv_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.0</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>In an apparent first , Iran and Israel engage ...</td>\n",
       "      <td>engage</td>\n",
       "      <td>slap</td>\n",
       "      <td>In an apparent first , Iran and Israel slap ea...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[0.17871094, -0.13671875, 0.091308594, 0.27148...</td>\n",
       "      <td>[-0.16085683, 0.11282813, -0.017742654, 0.1522...</td>\n",
       "      <td>[-0.15556335, 0.12433208, -0.025668057, 0.1493...</td>\n",
       "      <td>[0.115234375, -0.24853516, 0.05419922, 0.14550...</td>\n",
       "      <td>[0.20881, 0.13581, -0.34811, 0.10243, -0.44111...</td>\n",
       "      <td>[-0.29926562, 0.042028673, -0.24126922, -0.067...</td>\n",
       "      <td>[-0.2568383, 0.004592164, -0.21411426, -0.0841...</td>\n",
       "      <td>[0.138641, -0.26598, -0.23705998, 0.71699, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8832.0</td>\n",
       "      <td>All 22 &lt;promises/&gt; Trump made in his speech to...</td>\n",
       "      <td>sounds</td>\n",
       "      <td>22200</td>\n",
       "      <td>1.2</td>\n",
       "      <td>All 22 promises Trump made in his speech to Co...</td>\n",
       "      <td>promises</td>\n",
       "      <td>sounds</td>\n",
       "      <td>All 22 sounds Trump made in his speech to Cong...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>[0.15722656, -0.015136719, 0.024169922, 0.1142...</td>\n",
       "      <td>[-0.16917509, 0.11653765, -0.004016353, 0.1354...</td>\n",
       "      <td>[-0.16694859, 0.11431636, 0.0022720026, 0.1258...</td>\n",
       "      <td>[0.029296875, -0.31591797, -0.26293945, 0.2060...</td>\n",
       "      <td>[0.072236, -0.064091, -0.72274, 0.029478, -0.1...</td>\n",
       "      <td>[-0.25548378, 0.0055763684, -0.21668643, -0.01...</td>\n",
       "      <td>[-0.24318196, 0.0027593398, -0.2431335, -0.041...</td>\n",
       "      <td>[-0.274745, 0.17397, -0.23278001, 0.66924, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12174.0</td>\n",
       "      <td>New DOJ alert system will flag &lt;crimes/&gt; again...</td>\n",
       "      <td>laughter</td>\n",
       "      <td>32100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>New DOJ alert system will flag crimes against ...</td>\n",
       "      <td>crimes</td>\n",
       "      <td>laughter</td>\n",
       "      <td>New DOJ alert system will flag laughter agains...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>[0.31835938, -0.25976562, 0.19628906, 0.484375...</td>\n",
       "      <td>[-0.179599, 0.12229004, -0.008529663, 0.129858...</td>\n",
       "      <td>[-0.1853861, 0.12889583, -0.013356279, 0.11729...</td>\n",
       "      <td>[0.33666992, -0.56640625, -0.040039062, 0.5917...</td>\n",
       "      <td>[-0.23031, -0.030609, 0.082003, 0.13315, -0.00...</td>\n",
       "      <td>[-0.24717303, 0.081990376, -0.31995907, -0.066...</td>\n",
       "      <td>[-0.26802576, 0.06752585, -0.35055396, -0.0671...</td>\n",
       "      <td>[-0.251844, -0.38553903, 0.109877996, 0.428259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14191.0</td>\n",
       "      <td>Dutch minister resigns in drug baron &lt;row/&gt;</td>\n",
       "      <td>blow</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dutch minister resigns in drug baron row</td>\n",
       "      <td>row</td>\n",
       "      <td>blow</td>\n",
       "      <td>Dutch minister resigns in drug baron blow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.26953125, 0.07128906, 0.14746094, -0.083984...</td>\n",
       "      <td>[-0.21529505, 0.09837018, 0.0059074634, 0.1422...</td>\n",
       "      <td>[-0.20689122, 0.09824865, 0.00816076, 0.138844...</td>\n",
       "      <td>[0.1821289, -0.021484375, 0.21435547, -0.19335...</td>\n",
       "      <td>[0.7799, -0.012848, -0.63467, -0.20527, -0.251...</td>\n",
       "      <td>[-0.24108344, -0.05642715, -0.26649308, -0.049...</td>\n",
       "      <td>[-0.22866751, -0.009843903, -0.27912727, -0.07...</td>\n",
       "      <td>[0.19519001, -0.198688, -0.76445, 0.18273, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14268.0</td>\n",
       "      <td>Dozens dead in possible gas &lt;attack/&gt; in Syria...</td>\n",
       "      <td>bloating</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dozens dead in possible gas attack in Syria ; ...</td>\n",
       "      <td>attack</td>\n",
       "      <td>bloating</td>\n",
       "      <td>Dozens dead in possible gas bloating in Syria ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.012634277, 0.25585938, -0.1796875, 0.161132...</td>\n",
       "      <td>[-0.16217759, 0.09522083, 0.01645795, 0.153305...</td>\n",
       "      <td>[-0.16275138, 0.09460449, 0.021258319, 0.14807...</td>\n",
       "      <td>[-0.16802979, 0.12402344, -0.25683594, 0.22216...</td>\n",
       "      <td>[-0.0010849, 0.12138, 0.22883, 0.13895, -0.202...</td>\n",
       "      <td>[-0.28534657, 0.12610303, -0.32908022, -0.0660...</td>\n",
       "      <td>[-0.27154708, 0.1449134, -0.33190408, -0.07578...</td>\n",
       "      <td>[1.09489, 0.19946003, 0.109608, -0.25891, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>17.0</td>\n",
       "      <td>Eyeing an election , Austria 's far-right Free...</td>\n",
       "      <td>flirts</td>\n",
       "      <td>21100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Eyeing an election , Austria 's far-right Free...</td>\n",
       "      <td>commits</td>\n",
       "      <td>flirts</td>\n",
       "      <td>Eyeing an election , Austria 's far-right Free...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[0.15625, -0.12011719, -0.035888672, 0.0625, -...</td>\n",
       "      <td>[-0.19297282, 0.119241156, 0.0052346624, 0.128...</td>\n",
       "      <td>[-0.19415405, 0.12365723, 0.007941894, 0.12312...</td>\n",
       "      <td>[0.09643555, -0.4970703, -0.5515137, 0.0007324...</td>\n",
       "      <td>[-0.35989, 0.39939, 0.304, 0.22194, -0.21962, ...</td>\n",
       "      <td>[-0.26238957, 0.002128861, -0.291182, -0.06419...</td>\n",
       "      <td>[-0.27710137, -0.010364528, -0.32532543, -0.05...</td>\n",
       "      <td>[-0.363321, 0.166347, 0.457223, 0.43596, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>6829.0</td>\n",
       "      <td>The Latest : McCabe lawyer says criminal &lt;refe...</td>\n",
       "      <td>mind</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>The Latest : McCabe lawyer says criminal refer...</td>\n",
       "      <td>referral</td>\n",
       "      <td>mind</td>\n",
       "      <td>The Latest : McCabe lawyer says criminal mind ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[0.23535156, 0.1640625, 0.03930664, 0.15722656...</td>\n",
       "      <td>[-0.19099759, 0.12131135, 0.0032176143, 0.1335...</td>\n",
       "      <td>[-0.1856874, 0.10970147, 0.0059516374, 0.14110...</td>\n",
       "      <td>[0.45703125, 0.14160156, -0.1550293, 0.0146484...</td>\n",
       "      <td>[-0.19272, -0.3462, -0.16195, 0.12966, 0.48245...</td>\n",
       "      <td>[-0.2296312, 0.037876945, -0.30486488, -0.1363...</td>\n",
       "      <td>[-0.1922786, 0.08629804, -0.27266732, -0.16186...</td>\n",
       "      <td>[0.2517, -0.68961, -0.7339, -0.3505, -0.169030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>12145.0</td>\n",
       "      <td>Steve Bannon ’s own words show sharp break on ...</td>\n",
       "      <td>guards</td>\n",
       "      <td>21000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Steve Bannon ’s own words show sharp break on ...</td>\n",
       "      <td>issues</td>\n",
       "      <td>guards</td>\n",
       "      <td>Steve Bannon ’s own words show sharp break on ...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>[-0.040039062, 0.36914062, 0.15722656, -0.0066...</td>\n",
       "      <td>[-0.17600851, 0.10289748, 1.0388962e-05, 0.140...</td>\n",
       "      <td>[-0.17773305, 0.10385662, 0.0008969514, 0.1395...</td>\n",
       "      <td>[0.0007324219, 0.1171875, 0.37304688, -0.31521...</td>\n",
       "      <td>[0.33292, 0.1669, 0.11604, -0.046371, -0.36552...</td>\n",
       "      <td>[-0.18546198, -0.0034434667, -0.23186754, -0.0...</td>\n",
       "      <td>[-0.2327396, -0.01566634, -0.24392949, -0.0567...</td>\n",
       "      <td>[0.235443, 0.3515206, -0.308492, -0.200829, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>6845.0</td>\n",
       "      <td>Spicer : We do n't regret repeating claim that...</td>\n",
       "      <td>licked</td>\n",
       "      <td>11000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Spicer : We do n't regret repeating claim that...</td>\n",
       "      <td>spied</td>\n",
       "      <td>licked</td>\n",
       "      <td>Spicer : We do n't regret repeating claim that...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[-0.1796875, 0.1171875, -0.018676758, 0.135742...</td>\n",
       "      <td>[-0.18470407, 0.11734853, -0.020107837, 0.1634...</td>\n",
       "      <td>[-0.17765427, 0.11036682, -0.014645894, 0.1591...</td>\n",
       "      <td>[-0.28027344, -0.15234375, -0.20227051, 0.1661...</td>\n",
       "      <td>[-0.50318, 0.64528, -0.69668, -0.088312, -0.27...</td>\n",
       "      <td>[-0.32987472, -0.00243384, -0.21117866, 0.0087...</td>\n",
       "      <td>[-0.34803858, 0.015613832, -0.23332469, 0.0098...</td>\n",
       "      <td>[0.199165, 0.36876002, -0.065579, 0.29648098, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>4440.0</td>\n",
       "      <td>Flynn has promised Special Counsel ' full &lt;coo...</td>\n",
       "      <td>monty</td>\n",
       "      <td>21100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Flynn has promised Special Counsel ' full coop...</td>\n",
       "      <td>cooperation</td>\n",
       "      <td>monty</td>\n",
       "      <td>Flynn has promised Special Counsel ' full mont...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[0.11230469, -0.06738281, 0.10595703, 0.174804...</td>\n",
       "      <td>[-0.14827983, 0.10121053, 0.019482423, 0.13080...</td>\n",
       "      <td>[-0.14503396, 0.10284313, 0.022938121, 0.12385...</td>\n",
       "      <td>[0.3544922, -0.15185547, -0.05517578, 0.109375...</td>\n",
       "      <td>[-0.22355, 0.51565, -0.89353, 0.35083, 0.22297...</td>\n",
       "      <td>[-0.19999373, -0.019947642, -0.23654592, -0.08...</td>\n",
       "      <td>[-0.17993411, 0.008168418, -0.25023958, -0.108...</td>\n",
       "      <td>[-0.012869999, -0.36091602, 0.352424, 0.51513,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9479 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           original      edit  \\\n",
       "3        76.0  In an apparent first , Iran and Israel <engage...      slap   \n",
       "5      8832.0  All 22 <promises/> Trump made in his speech to...    sounds   \n",
       "6     12174.0  New DOJ alert system will flag <crimes/> again...  laughter   \n",
       "9     14191.0       Dutch minister resigns in drug baron <row/>       blow   \n",
       "10    14268.0  Dozens dead in possible gas <attack/> in Syria...  bloating   \n",
       "...       ...                                                ...       ...   \n",
       "3013     17.0  Eyeing an election , Austria 's far-right Free...    flirts   \n",
       "3014   6829.0  The Latest : McCabe lawyer says criminal <refe...      mind   \n",
       "3015  12145.0  Steve Bannon ’s own words show sharp break on ...    guards   \n",
       "3021   6845.0  Spicer : We do n't regret repeating claim that...    licked   \n",
       "3023   4440.0  Flynn has promised Special Counsel ' full <coo...     monty   \n",
       "\n",
       "      grades  meanGrade                                              Z_raw  \\\n",
       "3      20000        0.4  In an apparent first , Iran and Israel engage ...   \n",
       "5      22200        1.2  All 22 promises Trump made in his speech to Co...   \n",
       "6      32100        1.2  New DOJ alert system will flag crimes against ...   \n",
       "9          0        0.0          Dutch minister resigns in drug baron row    \n",
       "10     22100        1.0  Dozens dead in possible gas attack in Syria ; ...   \n",
       "...      ...        ...                                                ...   \n",
       "3013   21100        0.8  Eyeing an election , Austria 's far-right Free...   \n",
       "3014   10000        0.2  The Latest : McCabe lawyer says criminal refer...   \n",
       "3015   21000        0.6  Steve Bannon ’s own words show sharp break on ...   \n",
       "3021   11000        0.4  Spicer : We do n't regret repeating claim that...   \n",
       "3023   21100        0.8  Flynn has promised Special Counsel ' full coop...   \n",
       "\n",
       "         replaced     W_raw  \\\n",
       "3          engage      slap   \n",
       "5        promises    sounds   \n",
       "6          crimes  laughter   \n",
       "9             row      blow   \n",
       "10         attack  bloating   \n",
       "...           ...       ...   \n",
       "3013      commits    flirts   \n",
       "3014     referral      mind   \n",
       "3015       issues    guards   \n",
       "3021        spied    licked   \n",
       "3023  cooperation     monty   \n",
       "\n",
       "                                                  X_raw    Y  \\\n",
       "3     In an apparent first , Iran and Israel slap ea...  0.4   \n",
       "5     All 22 sounds Trump made in his speech to Cong...  1.2   \n",
       "6     New DOJ alert system will flag laughter agains...  1.2   \n",
       "9            Dutch minister resigns in drug baron blow   0.0   \n",
       "10    Dozens dead in possible gas bloating in Syria ...  1.0   \n",
       "...                                                 ...  ...   \n",
       "3013  Eyeing an election , Austria 's far-right Free...  0.8   \n",
       "3014  The Latest : McCabe lawyer says criminal mind ...  0.2   \n",
       "3015  Steve Bannon ’s own words show sharp break on ...  0.6   \n",
       "3021  Spicer : We do n't regret repeating claim that...  0.4   \n",
       "3023  Flynn has promised Special Counsel ' full mont...  0.8   \n",
       "\n",
       "                                                  W_vec  \\\n",
       "3     [0.17871094, -0.13671875, 0.091308594, 0.27148...   \n",
       "5     [0.15722656, -0.015136719, 0.024169922, 0.1142...   \n",
       "6     [0.31835938, -0.25976562, 0.19628906, 0.484375...   \n",
       "9     [0.26953125, 0.07128906, 0.14746094, -0.083984...   \n",
       "10    [0.012634277, 0.25585938, -0.1796875, 0.161132...   \n",
       "...                                                 ...   \n",
       "3013  [0.15625, -0.12011719, -0.035888672, 0.0625, -...   \n",
       "3014  [0.23535156, 0.1640625, 0.03930664, 0.15722656...   \n",
       "3015  [-0.040039062, 0.36914062, 0.15722656, -0.0066...   \n",
       "3021  [-0.1796875, 0.1171875, -0.018676758, 0.135742...   \n",
       "3023  [0.11230469, -0.06738281, 0.10595703, 0.174804...   \n",
       "\n",
       "                                                  Z_vec  \\\n",
       "3     [-0.16085683, 0.11282813, -0.017742654, 0.1522...   \n",
       "5     [-0.16917509, 0.11653765, -0.004016353, 0.1354...   \n",
       "6     [-0.179599, 0.12229004, -0.008529663, 0.129858...   \n",
       "9     [-0.21529505, 0.09837018, 0.0059074634, 0.1422...   \n",
       "10    [-0.16217759, 0.09522083, 0.01645795, 0.153305...   \n",
       "...                                                 ...   \n",
       "3013  [-0.19297282, 0.119241156, 0.0052346624, 0.128...   \n",
       "3014  [-0.19099759, 0.12131135, 0.0032176143, 0.1335...   \n",
       "3015  [-0.17600851, 0.10289748, 1.0388962e-05, 0.140...   \n",
       "3021  [-0.18470407, 0.11734853, -0.020107837, 0.1634...   \n",
       "3023  [-0.14827983, 0.10121053, 0.019482423, 0.13080...   \n",
       "\n",
       "                                                  X_vec  \\\n",
       "3     [-0.15556335, 0.12433208, -0.025668057, 0.1493...   \n",
       "5     [-0.16694859, 0.11431636, 0.0022720026, 0.1258...   \n",
       "6     [-0.1853861, 0.12889583, -0.013356279, 0.11729...   \n",
       "9     [-0.20689122, 0.09824865, 0.00816076, 0.138844...   \n",
       "10    [-0.16275138, 0.09460449, 0.021258319, 0.14807...   \n",
       "...                                                 ...   \n",
       "3013  [-0.19415405, 0.12365723, 0.007941894, 0.12312...   \n",
       "3014  [-0.1856874, 0.10970147, 0.0059516374, 0.14110...   \n",
       "3015  [-0.17773305, 0.10385662, 0.0008969514, 0.1395...   \n",
       "3021  [-0.17765427, 0.11036682, -0.014645894, 0.1591...   \n",
       "3023  [-0.14503396, 0.10284313, 0.022938121, 0.12385...   \n",
       "\n",
       "                                    W_init_sub_edit_vec  \\\n",
       "3     [0.115234375, -0.24853516, 0.05419922, 0.14550...   \n",
       "5     [0.029296875, -0.31591797, -0.26293945, 0.2060...   \n",
       "6     [0.33666992, -0.56640625, -0.040039062, 0.5917...   \n",
       "9     [0.1821289, -0.021484375, 0.21435547, -0.19335...   \n",
       "10    [-0.16802979, 0.12402344, -0.25683594, 0.22216...   \n",
       "...                                                 ...   \n",
       "3013  [0.09643555, -0.4970703, -0.5515137, 0.0007324...   \n",
       "3014  [0.45703125, 0.14160156, -0.1550293, 0.0146484...   \n",
       "3015  [0.0007324219, 0.1171875, 0.37304688, -0.31521...   \n",
       "3021  [-0.28027344, -0.15234375, -0.20227051, 0.1661...   \n",
       "3023  [0.3544922, -0.15185547, -0.05517578, 0.109375...   \n",
       "\n",
       "                                              W_glv_vec  \\\n",
       "3     [0.20881, 0.13581, -0.34811, 0.10243, -0.44111...   \n",
       "5     [0.072236, -0.064091, -0.72274, 0.029478, -0.1...   \n",
       "6     [-0.23031, -0.030609, 0.082003, 0.13315, -0.00...   \n",
       "9     [0.7799, -0.012848, -0.63467, -0.20527, -0.251...   \n",
       "10    [-0.0010849, 0.12138, 0.22883, 0.13895, -0.202...   \n",
       "...                                                 ...   \n",
       "3013  [-0.35989, 0.39939, 0.304, 0.22194, -0.21962, ...   \n",
       "3014  [-0.19272, -0.3462, -0.16195, 0.12966, 0.48245...   \n",
       "3015  [0.33292, 0.1669, 0.11604, -0.046371, -0.36552...   \n",
       "3021  [-0.50318, 0.64528, -0.69668, -0.088312, -0.27...   \n",
       "3023  [-0.22355, 0.51565, -0.89353, 0.35083, 0.22297...   \n",
       "\n",
       "                                              Z_glv_vec  \\\n",
       "3     [-0.29926562, 0.042028673, -0.24126922, -0.067...   \n",
       "5     [-0.25548378, 0.0055763684, -0.21668643, -0.01...   \n",
       "6     [-0.24717303, 0.081990376, -0.31995907, -0.066...   \n",
       "9     [-0.24108344, -0.05642715, -0.26649308, -0.049...   \n",
       "10    [-0.28534657, 0.12610303, -0.32908022, -0.0660...   \n",
       "...                                                 ...   \n",
       "3013  [-0.26238957, 0.002128861, -0.291182, -0.06419...   \n",
       "3014  [-0.2296312, 0.037876945, -0.30486488, -0.1363...   \n",
       "3015  [-0.18546198, -0.0034434667, -0.23186754, -0.0...   \n",
       "3021  [-0.32987472, -0.00243384, -0.21117866, 0.0087...   \n",
       "3023  [-0.19999373, -0.019947642, -0.23654592, -0.08...   \n",
       "\n",
       "                                              X_glv_vec  \\\n",
       "3     [-0.2568383, 0.004592164, -0.21411426, -0.0841...   \n",
       "5     [-0.24318196, 0.0027593398, -0.2431335, -0.041...   \n",
       "6     [-0.26802576, 0.06752585, -0.35055396, -0.0671...   \n",
       "9     [-0.22866751, -0.009843903, -0.27912727, -0.07...   \n",
       "10    [-0.27154708, 0.1449134, -0.33190408, -0.07578...   \n",
       "...                                                 ...   \n",
       "3013  [-0.27710137, -0.010364528, -0.32532543, -0.05...   \n",
       "3014  [-0.1922786, 0.08629804, -0.27266732, -0.16186...   \n",
       "3015  [-0.2327396, -0.01566634, -0.24392949, -0.0567...   \n",
       "3021  [-0.34803858, 0.015613832, -0.23332469, 0.0098...   \n",
       "3023  [-0.17993411, 0.008168418, -0.25023958, -0.108...   \n",
       "\n",
       "                                W_init_sub_edit_glv_vec  \n",
       "3     [0.138641, -0.26598, -0.23705998, 0.71699, -0....  \n",
       "5     [-0.274745, 0.17397, -0.23278001, 0.66924, -0....  \n",
       "6     [-0.251844, -0.38553903, 0.109877996, 0.428259...  \n",
       "9     [0.19519001, -0.198688, -0.76445, 0.18273, -0....  \n",
       "10    [1.09489, 0.19946003, 0.109608, -0.25891, -0.4...  \n",
       "...                                                 ...  \n",
       "3013  [-0.363321, 0.166347, 0.457223, 0.43596, -0.38...  \n",
       "3014  [0.2517, -0.68961, -0.7339, -0.3505, -0.169030...  \n",
       "3015  [0.235443, 0.3515206, -0.308492, -0.200829, -0...  \n",
       "3021  [0.199165, 0.36876002, -0.065579, 0.29648098, ...  \n",
       "3023  [-0.012869999, -0.36091602, 0.352424, 0.51513,...  \n",
       "\n",
       "[9479 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "total_df['W_vec'] = total_df['W_raw'].apply(lambda x: word_vector(model, x))\n",
    "total_df['Z_vec'] = total_df['Z_raw'].apply(lambda x: document_vector(model, x))\n",
    "total_df['X_vec'] = total_df['X_raw'].apply(lambda x: document_vector(model, x))\n",
    "total_df['W_init_sub_edit_vec'] = total_df.apply(edits_diff, axis=1)\n",
    "# glove vectors\n",
    "total_df['W_glv_vec'] = total_df['W_raw'].apply(lambda x: word_vector(glove_model, clean_string(x)))\n",
    "total_df['Z_glv_vec'] = total_df['Z_raw'].apply(lambda x: document_vector(glove_model, clean_string(x)))\n",
    "total_df['X_glv_vec'] = total_df['X_raw'].apply(lambda x: document_vector(glove_model, clean_string(x)))\n",
    "\n",
    "total_df['W_init_sub_edit_glv_vec'] = total_df.apply(lambda x: edits_diff(x, model=glove_model), axis=1)\n",
    "\n",
    "total_df = total_df.dropna()\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_glv_vec = total_df['W_glv_vec'].values\n",
    "W_glv_vec = np.concatenate(W_glv_vec).ravel().reshape(len(W_glv_vec), -1)\n",
    "clusterer = KMeans(n_clusters=20, random_state=10)\n",
    "total_df['cluster_labels'] = clusterer.fit_predict(W_glv_vec)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.array(total_df['cluster_labels']).reshape(-1,1))\n",
    "def one_hot(x):\n",
    "    return enc.transform(np.array([x]).reshape(-1,1)).toarray()[0]\n",
    "total_df['cluster_labels_one_hot'] = total_df['cluster_labels'].apply(lambda x: one_hot(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for i in range(20):\n",
    "    clusters[i] = total_df[total_df['cluster_labels']==i]['W_raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Phis: hypothsis testing based on Hossain et al.\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hossain et al, who compiled the Homicroedit dataset, analyzed and hypothesized what \"humor features\" exist in the humorous edits. Some of the candidate explainers were:\n",
    "#### Manual inspection\n",
    "1. Edit word forms a meaningful n-gram with adjacent words\n",
    "2. Connection b/w edit word and original word (e.g. semnatically distant, or similar pronounciation)\n",
    "3. Edit word makes a strong connection with an entity in the headline (Trump-hair, Obama-ears)\n",
    "4. Sarcasm\n",
    "5. Tension Supression/Relief Theory of Humor\n",
    "6. Edit word creates incongruity\n",
    "7. Setup and punchline: surprising edit at the end of the sentence\n",
    "-----------------\n",
    "### Translation to our setup\n",
    "1. Length of resulting edited sentence (should not vary with w)\n",
    "2. Cosine distance b/w Glove of edit word and the rest of words in sentence\n",
    "3. Location index of replaced word (should not vary with w)\n",
    "4. Sentiment polarity of edit word\n",
    "5. Sentiment polarity of resulting sentence\n",
    "6. Cosine distance b/w Glove of edited word and Glove of original word\n",
    "7. Cosine distance from neigbouring words\n",
    "8. Distance of final sentence from cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Length of resulting edited sentence (should not vary with w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['phis0'] = total_df['X_raw'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cosine distance b/w Glove of edit word and the rest of words in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_dist_edit_rest(x):\n",
    "    edit = x['edit']\n",
    "    edit_vec = x['W_glv_vec']\n",
    "    sent = x['X_raw']\n",
    "    dists = []\n",
    "    for word in sent.split(\" \"):\n",
    "        if word!=edit:\n",
    "            word_vec = word_vector(glove_model, clean_string(word))\n",
    "            if not np.isnan(word_vec).any():\n",
    "                dist = cosine_similarity(\n",
    "                    word_vec.reshape(1,-1), \n",
    "                    edit_vec.reshape(1,-1))\n",
    "                dists.append(dist)\n",
    "    return np.mean(dists)\n",
    "\n",
    "total_df['phis1'] = total_df.apply(avg_dist_edit_rest, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. location index of replaced word (should not vary with w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_of_edit(x):\n",
    "    return x['X_raw'].split(\" \").index(x['edit'])\n",
    "\n",
    "total_df['phis2'] = total_df.apply(ind_of_edit, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sentiment polarity of edit word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-04 13:08:59 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| sentiment | sstplus |\n",
      "=======================\n",
      "\n",
      "2021-06-04 13:08:59 INFO: Use device: cpu\n",
      "2021-06-04 13:08:59 INFO: Loading: tokenize\n",
      "2021-06-04 13:08:59 INFO: Loading: sentiment\n",
      "2021-06-04 13:09:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "\n",
    "def sentiment_of_word(x):\n",
    "    sents = []\n",
    "    doc = nlp(x)\n",
    "    for sentence in doc.sentences:\n",
    "        sent = sentence.sentiment\n",
    "        sents.append(sent)\n",
    "    return sents[0]\n",
    "\n",
    "total_df['phis3'] = total_df['edit'].apply(sentiment_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sentiment polarity of resulting sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_of_word(x):\n",
    "    sents = []\n",
    "    doc = nlp(x)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sent = sentence.sentiment\n",
    "        sents.append(sent)\n",
    "    return sents[0]\n",
    "\n",
    "total_df['phis4'] = total_df['X_raw'].apply(sentiment_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cosine distance b/w Glove of edited word and Glove of original word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['replaced_glv_vec'] = total_df['replaced'].apply(lambda x: word_vector(glove_model, clean_string(x)))\n",
    "total_df.dropna(inplace=True)\n",
    "def cosine_distance_original_and_edit_word(x):\n",
    "    edit_vec = x['W_glv_vec']\n",
    "    original_vec = x['replaced_glv_vec']\n",
    "    return cosine_similarity(\n",
    "                    edit_vec.reshape(1,-1), \n",
    "                    original_vec.reshape(1,-1))\n",
    "\n",
    "total_df['phis5'] = total_df.apply(cosine_distance_original_and_edit_word, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Cosine distance from neigbouring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_neighb(x, neighb=1):\n",
    "    edit_vec = x['W_glv_vec']\n",
    "    sentence = x['X_raw']\n",
    "    edit_index = x['phis2']\n",
    "    neighb_word_vec = word_vector(glove_model, sentence[edit_index+neighb])\n",
    "    if not np.isnan(neighb_word_vec).any():\n",
    "        similar = cosine_similarity(\n",
    "                        neighb_word_vec.reshape(1,-1), \n",
    "                        edit_vec.reshape(1,-1)).squeeze(axis=0)[0]\n",
    "        return similar\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "total_df['phis6'] = total_df.apply(dist_from_neighb, neighb=-2, axis=1)\n",
    "total_df['phis7'] = total_df.apply(dist_from_neighb, neighb=-1, axis=1)\n",
    "total_df['phis8'] = total_df.apply(dist_from_neighb, neighb=1, axis=1)\n",
    "total_df['phis9'] = total_df.apply(dist_from_neighb, neighb=2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Distance of final sentence from cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49 s, sys: 1.33 s, total: 50.3 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "cluster_centers = clusterer.cluster_centers_\n",
    "avs_resulting_sent_vec = total_df['X_raw'].apply(lambda x: document_vector(glove_model, clean_string(x)))\n",
    "distance_avg_sent_to_centroids = {10+i: [] for i in range(20)}\n",
    "for sent in avs_resulting_sent_vec:\n",
    "    for centroid_id, centroid in enumerate(cluster_centers):\n",
    "        distance_avg_sent_to_centroids[centroid_id+10].append(\n",
    "            cosine_similarity(sent.reshape(1,-1), \n",
    "                              centroid.reshape(1,-1)).item())\n",
    "for key in distance_avg_sent_to_centroids.keys():\n",
    "    total_df['phis'+str(key)] = distance_avg_sent_to_centroids[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(total_df.columns[total_df.columns.str.contains('phis')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7575"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize phis and drop nans\n",
    "for phi_col in list(total_df.columns[total_df.columns.str.contains('phis')]):\n",
    "    total_df[phi_col] = scale(total_df[phi_col])\n",
    "    \n",
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6857, 949) (718, 949)\n"
     ]
    }
   ],
   "source": [
    "unseen_clusters = 11\n",
    "total_df_seen = total_df[total_df['cluster_labels'] != unseen_clusters]\n",
    "total_df_unseen = total_df[total_df['cluster_labels'] == unseen_clusters]\n",
    "print(total_df_seen.shape, total_df_unseen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundtruth Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights_phis_groundtruth = np.random.uniform(-1, 1, 30)\n",
    "zero_out_inds = np.random.choice(30, 9, replace=False)\n",
    "weights_phis_groundtruth[zero_out_inds] = 0\n",
    "# make sure at least one not partic. is varying w/ W\n",
    "weights_phis_groundtruth[3] = 0\n",
    "neg_inds = np.random.choice(30, 15, replace=False)\n",
    "weights_phis_groundtruth[neg_inds] *= -1\n",
    "\n",
    "def generate_groudtruth(x, weight_to_apply):\n",
    "    phis = x[list(x.index[x.index.str.contains('phis')])].values\n",
    "    try:\n",
    "        result = (weight_to_apply@phis).squeeze(axis=0)\n",
    "    except:\n",
    "        result = (weight_to_apply@phis)\n",
    "    # add noise to linear combin.\n",
    "    result += np.random.normal(0, .5)\n",
    "    return result\n",
    "    \n",
    "total_df['Y_groundtruth'] = total_df.apply(lambda x: generate_groudtruth(x, weights_phis_groundtruth), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25091976,  0.90142861, -0.46398788,  0.        , -0.        ,\n",
       "       -0.        ,  0.88383278,  0.73235229, -0.20223002,  0.        ,\n",
       "       -0.        ,  0.9398197 ,  0.66488528, -0.57532178, -0.63635007,\n",
       "        0.        , -0.        ,  0.04951286,  0.13610996,  0.41754172,\n",
       "        0.22370579, -0.        ,  0.        , -0.26727631,  0.08786003,\n",
       "        0.57035192, -0.60065244, -0.        , -0.18482914,  0.90709917])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_phis_groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure dataframe: from single column multi-dimensional \n",
    "# to multi-column one-dimensional\n",
    "total_df[['X_glv_vec'+str(i) for i in range(len(total_df['X_glv_vec'].iloc[0]))]] =\\\n",
    "    pd.DataFrame(total_df.X_glv_vec.tolist(), index = total_df.index)\n",
    "total_df.drop('X_glv_vec', axis=1, inplace=True)\n",
    "\n",
    "total_df[['Z_glv_vec'+str(i) for i in range(len(total_df['Z_glv_vec'].iloc[0]))]] =\\\n",
    "    pd.DataFrame(total_df.Z_glv_vec.tolist(), index = total_df.index)\n",
    "total_df.drop('Z_glv_vec', axis=1, inplace=True)\n",
    "\n",
    "total_df[['W_glv_vec'+str(i) for i in range(len(total_df['W_glv_vec'].iloc[0]))]] =\\\n",
    "    pd.DataFrame(total_df.W_glv_vec.tolist(), index = total_df.index)\n",
    "total_df.drop('W_glv_vec', axis=1, inplace=True)\n",
    "\n",
    "# total_df[['cluster_labels_one_hot'+str(i) for i in range(len(total_df['cluster_labels_one_hot'].iloc[0]))]] =\\\n",
    "#     pd.DataFrame(total_df.cluster_labels_one_hot.tolist(), index = total_df.index)\n",
    "# total_df.drop('cluster_labels_one_hot', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6857, 949) (718, 949)\n",
      "(6171, 931) (686, 931) (574, 931) (144, 931)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# # choice = int(np.random.choice(5, 1, replace=False))\n",
    "# # dict_cluster_sizes = dict(total_df['cluster_labels'].value_counts())\n",
    "# # keys=list(dict_cluster_sizes.keys())\n",
    "# # values=list(dict_cluster_sizes.values())\n",
    "# # value = sorted(values)[15+choice]\n",
    "# # print(\"size of chosen split: \", value)\n",
    "# # unseen_clusters = [keys[values.index(value)]]\n",
    "\n",
    "# this is the cluster used for unseen W in paper\n",
    "# chosen via process above\n",
    "unseen_clusters = 11 \n",
    "\n",
    "# total_df_seen = total_df[~total_df['cluster_labels'].isin([unseen_clusters])]\n",
    "# total_df_unseen = total_df[total_df['cluster_labels'].isin([unseen_clusters])]\n",
    "\n",
    "total_df_seen = total_df[total_df['cluster_labels'] != unseen_clusters]\n",
    "total_df_unseen = total_df[total_df['cluster_labels'] == unseen_clusters]\n",
    "print(total_df_seen.shape, total_df_unseen.shape)\n",
    "\n",
    "total_df_seen = total_df_seen[\n",
    "                    list(total_df_seen.columns[\n",
    "                        total_df_seen.columns.str.contains('Z_glv_vec')])+\n",
    "                    list(total_df_seen.columns[\n",
    "                        total_df_seen.columns.str.contains('W_glv_vec')])+\n",
    "                    list(total_df_seen.columns[\n",
    "                        total_df_seen.columns.str.contains('X_glv_vec')])+['Y_groundtruth']\n",
    "                    + list(total_df_seen.columns[\n",
    "                        total_df_seen.columns.str.contains('phis')])\n",
    "]\n",
    "\n",
    "\n",
    "total_df_unseen = total_df_unseen[\n",
    "                    list(total_df_unseen.columns[\n",
    "                    total_df_unseen.columns.str.contains('Z_glv_vec')])+\n",
    "                    list(total_df_unseen.columns[\n",
    "                        total_df_unseen.columns.str.contains('W_glv_vec')])+\n",
    "                    list(total_df_unseen.columns[\n",
    "                        total_df_unseen.columns.str.contains('X_glv_vec')])+['Y_groundtruth']\n",
    "                    + list(total_df_unseen.columns[\n",
    "                        total_df_unseen.columns.str.contains('phis')])]\n",
    "\n",
    "total_df_seen = shuffle(total_df_seen)\n",
    "total_df_seen_train = total_df_seen.iloc[:int(len(total_df_seen)*0.9)]\n",
    "total_df_seen_test = total_df_seen.iloc[int(len(total_df_seen)*0.9):]\n",
    "\n",
    "total_df_unseen = shuffle(total_df_unseen)\n",
    "df_test_unseen_train = total_df_unseen.iloc[:int(len(total_df_unseen)*0.8)]\n",
    "df_test_unseen_test = total_df_unseen.iloc[int(len(total_df_unseen)*0.8):]\n",
    "\n",
    "total_df_seen = total_df_seen.to_numpy()\n",
    "total_df_seen_train = total_df_seen_train.to_numpy()\n",
    "total_df_seen_test = total_df_seen_test.to_numpy()\n",
    "\n",
    "testset_unseen = total_df_unseen.to_numpy()\n",
    "test_unseen_train = df_test_unseen_train.to_numpy()\n",
    "test_unseen_test = df_test_unseen_test.to_numpy()\n",
    "\n",
    "print(total_df_seen_train.shape, total_df_seen_test.shape, \n",
    "      test_unseen_train.shape, test_unseen_test.shape)\n",
    "\n",
    "Path('../../data/Humicroedit/').mkdir(parents=True, exist_ok=True)\n",
    "np.savez_compressed('../../data/Humicroedit/Humicroedit_trainset_seen_train.npz', total_df_seen_train)\n",
    "np.savez_compressed('../../data/Humicroedit/Humicroedit_trainset_seen_test.npz', total_df_seen_test)\n",
    "np.savez_compressed('../../data/Humicroedit/Humicroedit_testset_unseen_train.npz', test_unseen_train)\n",
    "np.savez_compressed('../../data/Humicroedit/Humicroedit_testset_unseen_test.npz', test_unseen_test)\n",
    "np.savez_compressed('../../data/Humicroedit/Humicroedit_params.npz', weights_phis_groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional experiments: create 100 versions of datasets with different coefficients for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_runs_diff_Y = 100\n",
    "for run in range(num_runs_diff_Y):\n",
    "    weights_phis_groundtruth = np.random.uniform(-1, 1, 30)\n",
    "    zero_out_inds = np.random.choice(30, 9, replace=False)\n",
    "    weights_phis_groundtruth[zero_out_inds] = 0\n",
    "    # make sure at least one not partic. is varying w/ W\n",
    "    weights_phis_groundtruth[3] = 0\n",
    "    neg_inds = np.random.choice(30, 15, replace=False)\n",
    "    weights_phis_groundtruth[neg_inds] *= -1\n",
    "\n",
    "    def generate_groudtruth(x, weight_to_apply):\n",
    "        phis = x[list(x.index[x.index.str.contains('phis')])].values\n",
    "        try:\n",
    "            result = (weight_to_apply@phis).squeeze(axis=0)\n",
    "        except:\n",
    "            result = (weight_to_apply@phis)\n",
    "        # add noise to linear combin.\n",
    "        result += np.random.normal(0, .5)\n",
    "        return result\n",
    "\n",
    "    total_df['Y_groundtruth'] = total_df.apply(lambda x: generate_groudtruth(x, weights_phis_groundtruth), axis=1)\n",
    "    \n",
    "    unseen_clusters = 11 \n",
    "\n",
    "    total_df_seen = total_df[~total_df['cluster_labels'].isin([unseen_clusters])]\n",
    "    total_df_unseen = total_df[total_df['cluster_labels'].isin([unseen_clusters])]\n",
    "#     print(total_df_seen.shape, total_df_unseen.shape)\n",
    "\n",
    "    total_df_seen = total_df_seen[\n",
    "                        list(total_df_seen.columns[\n",
    "                            total_df_seen.columns.str.contains('Z_glv_vec')])+\n",
    "                        list(total_df_seen.columns[\n",
    "                            total_df_seen.columns.str.contains('W_glv_vec')])+\n",
    "                        list(total_df_seen.columns[\n",
    "                            total_df_seen.columns.str.contains('X_glv_vec')])+['Y_groundtruth']\n",
    "                        + list(total_df_seen.columns[\n",
    "                            total_df_seen.columns.str.contains('phis')])\n",
    "    ]\n",
    "\n",
    "\n",
    "    total_df_unseen = total_df_unseen[\n",
    "                        list(total_df_unseen.columns[\n",
    "                        total_df_unseen.columns.str.contains('Z_glv_vec')])+\n",
    "                        list(total_df_unseen.columns[\n",
    "                            total_df_unseen.columns.str.contains('W_glv_vec')])+\n",
    "                        list(total_df_unseen.columns[\n",
    "                            total_df_unseen.columns.str.contains('X_glv_vec')])+['Y_groundtruth']\n",
    "                        + list(total_df_unseen.columns[\n",
    "                            total_df_unseen.columns.str.contains('phis')])]\n",
    "\n",
    "    total_df_seen = shuffle(total_df_seen)\n",
    "    total_df_seen_train = total_df_seen.iloc[:int(len(total_df_seen)*0.9)]\n",
    "    total_df_seen_test = total_df_seen.iloc[int(len(total_df_seen)*0.9):]\n",
    "\n",
    "    total_df_unseen = shuffle(total_df_unseen)\n",
    "    df_test_unseen_train = total_df_unseen.iloc[:int(len(total_df_unseen)*0.8)]\n",
    "    df_test_unseen_test = total_df_unseen.iloc[int(len(total_df_unseen)*0.8):]\n",
    "\n",
    "    total_df_seen = total_df_seen.to_numpy()\n",
    "    total_df_seen_train = total_df_seen_train.to_numpy()\n",
    "    total_df_seen_test = total_df_seen_test.to_numpy()\n",
    "\n",
    "    testset_unseen = total_df_unseen.to_numpy()\n",
    "    test_unseen_train = df_test_unseen_train.to_numpy()\n",
    "    test_unseen_test = df_test_unseen_test.to_numpy()\n",
    "\n",
    "#     print(total_df_seen_train.shape, total_df_seen_test.shape, \n",
    "#           test_unseen_train.shape, test_unseen_test.shape)\n",
    "    \n",
    "    Path('../../data/Humicroedit/diffY/').mkdir(parents=True, exist_ok=True)    \n",
    "    np.savez_compressed('../../data/Humicroedit/diffY/Humicroedit_unif{}_trainset_seen_train.npz'.format(run), total_df_seen_train)\n",
    "    np.savez_compressed('../../data/Humicroedit/diffY/Humicroedit_unif{}_trainset_seen_test.npz'.format(run), total_df_seen_test)\n",
    "    np.savez_compressed('../../data/Humicroedit/diffY/Humicroedit_unif{}_testset_unseen_train.npz'.format(run), test_unseen_train)\n",
    "    np.savez_compressed('../../data/Humicroedit/diffY/Humicroedit_unif{}_testset_unseen_test.npz'.format(run), test_unseen_test)\n",
    "    np.savez_compressed('../../data/Humicroedit/diffY/Humicroedit_unif{}_params.npz'.format(run), weights_phis_groundtruth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
